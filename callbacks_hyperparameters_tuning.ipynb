{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldnâ€™t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a4 in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.26.0)\n",
      "Requirement already satisfied: torch>=1.13 in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.1.1)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.8.0)\n",
      "Requirement already satisfied: opencv-python in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.8.1.78)\n",
      "Requirement already satisfied: pygame in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.15.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.6)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.66.1)\n",
      "Requirement already satisfied: rich in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (13.7.0)\n",
      "Requirement already satisfied: shimmy~=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (1.1.0)\n",
      "Requirement already satisfied: pillow in /opt/homebrew/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a4) (10.0.1)\n",
      "Requirement already satisfied: autorom~=0.6.1 in /opt/homebrew/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (0.6.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (8.1.7)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /opt/homebrew/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /opt/homebrew/lib/python3.11/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.59.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.5.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.16.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/lib/python3.11/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (6.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a4) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.13->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!apt install swig\n",
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 503       |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.7      |\n",
      "|    critic_loss     | 1.46      |\n",
      "|    ent_coef        | 0.811     |\n",
      "|    ent_coef_loss   | -0.346    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 466       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.3      |\n",
      "|    critic_loss     | 0.965     |\n",
      "|    ent_coef        | 0.645     |\n",
      "|    ent_coef_loss   | -0.661    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 457       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.7      |\n",
      "|    critic_loss     | 0.743     |\n",
      "|    ent_coef        | 0.52      |\n",
      "|    ent_coef_loss   | -0.804    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 454       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 78.5      |\n",
      "|    critic_loss     | 2.49      |\n",
      "|    ent_coef        | 0.429     |\n",
      "|    ent_coef_loss   | -0.642    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 453       |\n",
      "|    time_elapsed    | 8         |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 103       |\n",
      "|    critic_loss     | 4.01      |\n",
      "|    ent_coef        | 0.365     |\n",
      "|    ent_coef_loss   | -0.642    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 455       |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 112       |\n",
      "|    critic_loss     | 2.64      |\n",
      "|    ent_coef        | 0.312     |\n",
      "|    ent_coef_loss   | -0.428    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.31e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 457       |\n",
      "|    time_elapsed    | 12        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 133       |\n",
      "|    critic_loss     | 1.97      |\n",
      "|    ent_coef        | 0.25      |\n",
      "|    ent_coef_loss   | -1.02     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.24e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 458       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 142       |\n",
      "|    critic_loss     | 2.78      |\n",
      "|    ent_coef        | 0.2       |\n",
      "|    ent_coef_loss   | -0.597    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 459       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 161       |\n",
      "|    critic_loss     | 3.18      |\n",
      "|    ent_coef        | 0.181     |\n",
      "|    ent_coef_loss   | 0.0827    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 460       |\n",
      "|    time_elapsed    | 17        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 164       |\n",
      "|    critic_loss     | 2.85      |\n",
      "|    ent_coef        | 0.189     |\n",
      "|    ent_coef_loss   | 0.28      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    verbose=1,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-395.15 +/- 101.77\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.37e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 248       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.4      |\n",
      "|    critic_loss     | 0.272     |\n",
      "|    ent_coef        | 0.812     |\n",
      "|    ent_coef_loss   | -0.341    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 218       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 45.7      |\n",
      "|    critic_loss     | 0.168     |\n",
      "|    ent_coef        | 0.649     |\n",
      "|    ent_coef_loss   | -0.573    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.31e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 217       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 62.2      |\n",
      "|    critic_loss     | 0.253     |\n",
      "|    ent_coef        | 0.535     |\n",
      "|    ent_coef_loss   | -0.595    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.18e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 213       |\n",
      "|    time_elapsed    | 14        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 76.1      |\n",
      "|    critic_loss     | 0.544     |\n",
      "|    ent_coef        | 0.455     |\n",
      "|    ent_coef_loss   | -0.38     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -980     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 211      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 73       |\n",
      "|    critic_loss     | 0.456    |\n",
      "|    ent_coef        | 0.392    |\n",
      "|    ent_coef_loss   | -0.634   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -833     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 209      |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 77.1     |\n",
      "|    critic_loss     | 0.589    |\n",
      "|    ent_coef        | 0.33     |\n",
      "|    ent_coef_loss   | -0.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -741     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 211      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.8     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -0.828   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -663     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 213      |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78       |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | -0.555   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -603     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 67.6     |\n",
      "|    critic_loss     | 0.932    |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | -0.982   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -553     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 215      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 67.2     |\n",
      "|    critic_loss     | 0.929    |\n",
      "|    ent_coef        | 0.152    |\n",
      "|    ent_coef_loss   | -0.642   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=0,\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-152.42 +/- 84.35\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCallback(BaseCallback):\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super(SimpleCallback,self).__init__(verbose)\n",
    "        self._called = False\n",
    "\n",
    "    def _on_step(self):\n",
    "        if not self._called:\n",
    "            print(\"callback - first call\")\n",
    "            self._called = True\n",
    "            return True\n",
    "        print(\"callback - second call\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "callback - first call\n",
      "callback - second call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x16a730690>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "model.learn(8000, callback=SimpleCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, log_dir, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir,\"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path,exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            x, y = ts2xy(load_results(self.log_dir),\"timesteps\")\n",
    "            if len(x)>0:\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0: \n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    if self.verbose > 0 :\n",
    "                        print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                        print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "        return True\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 40\n",
      "Best mean reward: -inf - Last mean reward per episode: 37.00\n",
      "Saving new best model at 37 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 60\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 25.00\n",
      "Num timesteps: 80\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 23.67\n",
      "Num timesteps: 100\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 23.67\n",
      "Num timesteps: 120\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 23.20\n",
      "Num timesteps: 140\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 23.20\n",
      "Num timesteps: 160\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 22.86\n",
      "Num timesteps: 180\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 22.86\n",
      "Num timesteps: 200\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 22.86\n",
      "Num timesteps: 220\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 26.62\n",
      "Num timesteps: 240\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 26.62\n",
      "Num timesteps: 260\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.89\n",
      "Num timesteps: 280\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.20\n",
      "Num timesteps: 300\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.20\n",
      "Num timesteps: 320\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.55\n",
      "Num timesteps: 340\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.55\n",
      "Num timesteps: 360\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.55\n",
      "Num timesteps: 380\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.83\n",
      "Num timesteps: 400\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.46\n",
      "Num timesteps: 420\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.46\n",
      "Num timesteps: 440\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.14\n",
      "Num timesteps: 460\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.62\n",
      "Num timesteps: 480\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.62\n",
      "Num timesteps: 500\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.59\n",
      "Num timesteps: 520\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.59\n",
      "Num timesteps: 540\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.59\n",
      "Num timesteps: 560\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.72\n",
      "Num timesteps: 580\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.53\n",
      "Num timesteps: 600\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.80\n",
      "Num timesteps: 620\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.24\n",
      "Num timesteps: 640\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.24\n",
      "Num timesteps: 660\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.73\n",
      "Num timesteps: 680\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.73\n",
      "Num timesteps: 700\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.00\n",
      "Num timesteps: 720\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.00\n",
      "Num timesteps: 740\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.92\n",
      "Num timesteps: 760\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.00\n",
      "Num timesteps: 780\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.00\n",
      "Num timesteps: 800\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.00\n",
      "Num timesteps: 820\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.67\n",
      "Num timesteps: 840\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.43\n",
      "Num timesteps: 860\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.63\n",
      "Num timesteps: 880\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.63\n",
      "Num timesteps: 900\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.48\n",
      "Num timesteps: 920\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.48\n",
      "Num timesteps: 940\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.88\n",
      "Num timesteps: 960\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.09\n",
      "Num timesteps: 980\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.86\n",
      "Num timesteps: 1000\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.86\n",
      "Num timesteps: 1020\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 27.86\n",
      "Num timesteps: 1040\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.75\n",
      "Num timesteps: 1060\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.57\n",
      "Num timesteps: 1080\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.57\n",
      "Num timesteps: 1100\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 28.57\n",
      "Num timesteps: 1120\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.45\n",
      "Num timesteps: 1140\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.45\n",
      "Num timesteps: 1160\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.69\n",
      "Num timesteps: 1180\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.69\n",
      "Num timesteps: 1200\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 29.69\n",
      "Num timesteps: 1220\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.38\n",
      "Num timesteps: 1240\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.38\n",
      "Num timesteps: 1260\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.54\n",
      "Num timesteps: 1280\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.54\n",
      "Num timesteps: 1300\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 30.54\n",
      "Num timesteps: 1320\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 31.33\n",
      "Num timesteps: 1340\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 31.33\n",
      "Num timesteps: 1360\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 31.33\n",
      "Num timesteps: 1380\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 31.33\n",
      "Num timesteps: 1400\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.28\n",
      "Num timesteps: 1420\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.28\n",
      "Num timesteps: 1440\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.32\n",
      "Num timesteps: 1460\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.33\n",
      "Num timesteps: 1480\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.33\n",
      "Num timesteps: 1500\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.33\n",
      "Num timesteps: 1520\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.87\n",
      "Num timesteps: 1540\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.77\n",
      "Num timesteps: 1560\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.77\n",
      "Num timesteps: 1580\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.77\n",
      "Num timesteps: 1600\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.77\n",
      "Num timesteps: 1620\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 32.77\n",
      "Num timesteps: 1640\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 33.77\n",
      "Num timesteps: 1660\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 33.77\n",
      "Num timesteps: 1680\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 33.77\n",
      "Num timesteps: 1700\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.35\n",
      "Num timesteps: 1720\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.38\n",
      "Num timesteps: 1740\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1760\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1780\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1800\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1820\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1840\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 34.10\n",
      "Num timesteps: 1860\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 35.71\n",
      "Num timesteps: 1880\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 35.71\n",
      "Num timesteps: 1900\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 35.71\n",
      "Num timesteps: 1920\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 35.71\n",
      "Num timesteps: 1940\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 35.71\n",
      "Num timesteps: 1960\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 36.66\n",
      "Num timesteps: 1980\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 36.57\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 36.57\n",
      "Num timesteps: 2020\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 36.57\n",
      "Num timesteps: 2040\n",
      "Best mean reward: 37.00 - Last mean reward per episode: 37.09\n",
      "Saving new best model at 2040 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2060\n",
      "Best mean reward: 37.09 - Last mean reward per episode: 37.09\n",
      "Num timesteps: 2080\n",
      "Best mean reward: 37.09 - Last mean reward per episode: 37.11\n",
      "Saving new best model at 2078 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2100\n",
      "Best mean reward: 37.11 - Last mean reward per episode: 37.11\n",
      "Num timesteps: 2120\n",
      "Best mean reward: 37.11 - Last mean reward per episode: 37.11\n",
      "Num timesteps: 2140\n",
      "Best mean reward: 37.11 - Last mean reward per episode: 37.26\n",
      "Saving new best model at 2124 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2160\n",
      "Best mean reward: 37.26 - Last mean reward per episode: 37.26\n",
      "Num timesteps: 2180\n",
      "Best mean reward: 37.26 - Last mean reward per episode: 37.36\n",
      "Saving new best model at 2167 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2200\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2220\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2240\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2260\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2280\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2300\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.36\n",
      "Num timesteps: 2320\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 39.12\n",
      "Saving new best model at 2308 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2340\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 39.00\n",
      "Num timesteps: 2360\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 39.00\n",
      "Num timesteps: 2380\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 38.72\n",
      "Num timesteps: 2400\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 38.72\n",
      "Num timesteps: 2420\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 38.72\n",
      "Num timesteps: 2440\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 39.34\n",
      "Saving new best model at 2439 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2460\n",
      "Best mean reward: 39.34 - Last mean reward per episode: 38.90\n",
      "Num timesteps: 2480\n",
      "Best mean reward: 39.34 - Last mean reward per episode: 38.90\n",
      "Num timesteps: 2500\n",
      "Best mean reward: 39.34 - Last mean reward per episode: 38.90\n",
      "Num timesteps: 2520\n",
      "Best mean reward: 39.34 - Last mean reward per episode: 38.90\n",
      "Num timesteps: 2540\n",
      "Best mean reward: 39.34 - Last mean reward per episode: 39.48\n",
      "Saving new best model at 2527 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2560\n",
      "Best mean reward: 39.48 - Last mean reward per episode: 39.22\n",
      "Num timesteps: 2580\n",
      "Best mean reward: 39.48 - Last mean reward per episode: 39.22\n",
      "Num timesteps: 2600\n",
      "Best mean reward: 39.48 - Last mean reward per episode: 39.22\n",
      "Num timesteps: 2620\n",
      "Best mean reward: 39.48 - Last mean reward per episode: 39.22\n",
      "Num timesteps: 2640\n",
      "Best mean reward: 39.48 - Last mean reward per episode: 39.73\n",
      "Saving new best model at 2622 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2660\n",
      "Best mean reward: 39.73 - Last mean reward per episode: 39.73\n",
      "Num timesteps: 2680\n",
      "Best mean reward: 39.73 - Last mean reward per episode: 39.73\n",
      "Num timesteps: 2700\n",
      "Best mean reward: 39.73 - Last mean reward per episode: 39.73\n",
      "Num timesteps: 2720\n",
      "Best mean reward: 39.73 - Last mean reward per episode: 40.34\n",
      "Saving new best model at 2703 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2740\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.34\n",
      "Num timesteps: 2760\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.34\n",
      "Num timesteps: 2780\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.34\n",
      "Num timesteps: 2800\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.34\n",
      "Num timesteps: 2820\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.34\n",
      "Num timesteps: 2840\n",
      "Best mean reward: 40.34 - Last mean reward per episode: 40.99\n",
      "Saving new best model at 2828 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2860\n",
      "Best mean reward: 40.99 - Last mean reward per episode: 40.99\n",
      "Num timesteps: 2880\n",
      "Best mean reward: 40.99 - Last mean reward per episode: 41.13\n",
      "Saving new best model at 2879 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2900\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 2920\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 2940\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 2960\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 2980\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 3020\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 3040\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 3060\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 41.13\n",
      "Num timesteps: 3080\n",
      "Best mean reward: 41.13 - Last mean reward per episode: 43.11\n",
      "Saving new best model at 3061 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3100\n",
      "Best mean reward: 43.11 - Last mean reward per episode: 43.11\n",
      "Num timesteps: 3120\n",
      "Best mean reward: 43.11 - Last mean reward per episode: 43.21\n",
      "Saving new best model at 3111 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3140\n",
      "Best mean reward: 43.21 - Last mean reward per episode: 43.21\n",
      "Num timesteps: 3160\n",
      "Best mean reward: 43.21 - Last mean reward per episode: 43.21\n",
      "Num timesteps: 3180\n",
      "Best mean reward: 43.21 - Last mean reward per episode: 43.21\n",
      "Num timesteps: 3200\n",
      "Best mean reward: 43.21 - Last mean reward per episode: 43.21\n",
      "Num timesteps: 3220\n",
      "Best mean reward: 43.21 - Last mean reward per episode: 44.11\n",
      "Saving new best model at 3220 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3240\n",
      "Best mean reward: 44.11 - Last mean reward per episode: 44.11\n",
      "Num timesteps: 3260\n",
      "Best mean reward: 44.11 - Last mean reward per episode: 44.11\n",
      "Num timesteps: 3280\n",
      "Best mean reward: 44.11 - Last mean reward per episode: 44.11\n",
      "Num timesteps: 3300\n",
      "Best mean reward: 44.11 - Last mean reward per episode: 44.11\n",
      "Num timesteps: 3320\n",
      "Best mean reward: 44.11 - Last mean reward per episode: 44.74\n",
      "Saving new best model at 3311 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3340\n",
      "Best mean reward: 44.74 - Last mean reward per episode: 44.74\n",
      "Num timesteps: 3360\n",
      "Best mean reward: 44.74 - Last mean reward per episode: 44.63\n",
      "Num timesteps: 3380\n",
      "Best mean reward: 44.74 - Last mean reward per episode: 44.63\n",
      "Num timesteps: 3400\n",
      "Best mean reward: 44.74 - Last mean reward per episode: 44.63\n",
      "Num timesteps: 3420\n",
      "Best mean reward: 44.74 - Last mean reward per episode: 44.76\n",
      "Saving new best model at 3402 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3440\n",
      "Best mean reward: 44.76 - Last mean reward per episode: 44.76\n",
      "Num timesteps: 3460\n",
      "Best mean reward: 44.76 - Last mean reward per episode: 44.76\n",
      "Num timesteps: 3480\n",
      "Best mean reward: 44.76 - Last mean reward per episode: 44.76\n",
      "Num timesteps: 3500\n",
      "Best mean reward: 44.76 - Last mean reward per episode: 45.39\n",
      "Saving new best model at 3495 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3520\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 45.10\n",
      "Num timesteps: 3540\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 45.10\n",
      "Num timesteps: 3560\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 44.91\n",
      "Num timesteps: 3580\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 44.91\n",
      "Num timesteps: 3600\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 44.91\n",
      "Num timesteps: 3620\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 44.91\n",
      "Num timesteps: 3640\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 44.91\n",
      "Num timesteps: 3660\n",
      "Best mean reward: 45.39 - Last mean reward per episode: 45.66\n",
      "Saving new best model at 3653 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3680\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3700\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3720\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3740\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3760\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3780\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3800\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3820\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3840\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 45.66\n",
      "Num timesteps: 3860\n",
      "Best mean reward: 45.66 - Last mean reward per episode: 47.42\n",
      "Saving new best model at 3841 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3880\n",
      "Best mean reward: 47.42 - Last mean reward per episode: 47.42\n",
      "Num timesteps: 3900\n",
      "Best mean reward: 47.42 - Last mean reward per episode: 47.42\n",
      "Num timesteps: 3920\n",
      "Best mean reward: 47.42 - Last mean reward per episode: 47.67\n",
      "Saving new best model at 3909 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3940\n",
      "Best mean reward: 47.67 - Last mean reward per episode: 47.67\n",
      "Num timesteps: 3960\n",
      "Best mean reward: 47.67 - Last mean reward per episode: 47.67\n",
      "Num timesteps: 3980\n",
      "Best mean reward: 47.67 - Last mean reward per episode: 47.83\n",
      "Saving new best model at 3970 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 47.83 - Last mean reward per episode: 47.83\n",
      "Num timesteps: 4020\n",
      "Best mean reward: 47.83 - Last mean reward per episode: 47.83\n",
      "Num timesteps: 4040\n",
      "Best mean reward: 47.83 - Last mean reward per episode: 48.07\n",
      "Saving new best model at 4038 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4060\n",
      "Best mean reward: 48.07 - Last mean reward per episode: 48.07\n",
      "Num timesteps: 4080\n",
      "Best mean reward: 48.07 - Last mean reward per episode: 48.07\n",
      "Num timesteps: 4100\n",
      "Best mean reward: 48.07 - Last mean reward per episode: 48.19\n",
      "Saving new best model at 4096 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4120\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.19\n",
      "Num timesteps: 4140\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.19\n",
      "Num timesteps: 4160\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.19\n",
      "Num timesteps: 4180\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.19\n",
      "Num timesteps: 4200\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.19\n",
      "Num timesteps: 4220\n",
      "Best mean reward: 48.19 - Last mean reward per episode: 48.92\n",
      "Saving new best model at 4207 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4240\n",
      "Best mean reward: 48.92 - Last mean reward per episode: 48.92\n",
      "Num timesteps: 4260\n",
      "Best mean reward: 48.92 - Last mean reward per episode: 48.92\n",
      "Num timesteps: 4280\n",
      "Best mean reward: 48.92 - Last mean reward per episode: 48.92\n",
      "Num timesteps: 4300\n",
      "Best mean reward: 48.92 - Last mean reward per episode: 49.28\n",
      "Saving new best model at 4287 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4320\n",
      "Best mean reward: 49.28 - Last mean reward per episode: 49.28\n",
      "Num timesteps: 4340\n",
      "Best mean reward: 49.28 - Last mean reward per episode: 49.28\n",
      "Num timesteps: 4360\n",
      "Best mean reward: 49.28 - Last mean reward per episode: 49.42\n",
      "Saving new best model at 4349 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4380\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4400\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4420\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4440\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4460\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4480\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4500\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4520\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4540\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 49.42\n",
      "Num timesteps: 4560\n",
      "Best mean reward: 49.42 - Last mean reward per episode: 51.13\n",
      "Saving new best model at 4551 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4580\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4600\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4620\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4640\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4660\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4680\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4700\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4720\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4740\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4760\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 51.13\n",
      "Num timesteps: 4780\n",
      "Best mean reward: 51.13 - Last mean reward per episode: 53.00\n",
      "Saving new best model at 4770 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4800\n",
      "Best mean reward: 53.00 - Last mean reward per episode: 53.00\n",
      "Num timesteps: 4820\n",
      "Best mean reward: 53.00 - Last mean reward per episode: 53.00\n",
      "Num timesteps: 4840\n",
      "Best mean reward: 53.00 - Last mean reward per episode: 53.00\n",
      "Num timesteps: 4860\n",
      "Best mean reward: 53.00 - Last mean reward per episode: 53.33\n",
      "Saving new best model at 4853 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4880\n",
      "Best mean reward: 53.33 - Last mean reward per episode: 53.33\n",
      "Num timesteps: 4900\n",
      "Best mean reward: 53.33 - Last mean reward per episode: 53.33\n",
      "Num timesteps: 4920\n",
      "Best mean reward: 53.33 - Last mean reward per episode: 53.33\n",
      "Num timesteps: 4940\n",
      "Best mean reward: 53.33 - Last mean reward per episode: 53.60\n",
      "Saving new best model at 4931 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4960\n",
      "Best mean reward: 53.60 - Last mean reward per episode: 53.60\n",
      "Num timesteps: 4980\n",
      "Best mean reward: 53.60 - Last mean reward per episode: 53.60\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 53.60 - Last mean reward per episode: 53.60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x286290110>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "env = make_vec_env(\"CartPole-v1\",n_envs=1,monitor_dir = log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='66a9b24a-3afd-489a-8b71-f75eff6c2bee'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16c569210>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the performance in realtime.\n",
    "\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get the monitor's data\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if self._plot is None: # make the plot\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(6,3))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, = ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.show()\n",
    "        else: # update and rescale the plot\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "                                    self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True,True,True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "        \n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('MountainCarContinuous-v0', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "plotting_callback = PlottingCallback()\n",
    "        \n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(10000, callback=plotting_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:05<00:00, 365.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pbar):\n",
    "        super().__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps):  # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def __enter__(self):  # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "\n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "model = TD3(\"MlpPolicy\", \"Pendulum-v1\", verbose=0)\n",
    "# Using a context manager garanties that the tqdm progress bar closes correctly\n",
    "with ProgressBarManager(2000) as callback:\n",
    "    model.learn(2000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1590it [00:00, 7875.09it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: 21.41\n",
      "Saving new best model at 985 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 21.41 - Last mean reward per episode: 21.98\n",
      "Saving new best model at 2000 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1836.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "# Create callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "with ProgressBarManager(1000) as progress_callback:\n",
    "  # This is equivalent to callback=CallbackList([progress_callback, auto_save_callback])\n",
    "  model.learn(1000, callback=[progress_callback, auto_save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "class EvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "\n",
    "    :param eval_env: (gym.Env) The environment used for initialization\n",
    "    :param n_eval_episodes: (int) The number of episodes to test the agent\n",
    "    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        This method will be called by the model.\n",
    "\n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n_calls is automatically updated because\n",
    "        # we derive from BaseCallback\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # === YOUR CODE HERE ===#\n",
    "            # Evaluate the agent:\n",
    "            # you need to do self.n_eval_episodes loop using self.eval_env\n",
    "            # hint: you can use self.model.predict(obs, deterministic=True)\n",
    "            \n",
    "            \n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                obs = self.eval_env.reset()[0]\n",
    "                done = False\n",
    "                total_ep_reward = 0\n",
    "                while not done: \n",
    "                    action, a = self.model.predict(obs,deterministic=True)\n",
    "                    obs,reward,done, *_ = self.eval_env.step(action)\n",
    "                    total_ep_reward += reward\n",
    "                \n",
    "                if total_ep_reward>self.best_mean_reward:\n",
    "                    self.best_mean_reward = total_ep_reward\n",
    "                    self.model.save(\"best_model\") \n",
    "                    print(\"Best mean reward: {:.2f}\".format(self.best_mean_reward))\n",
    "            # Save the agent if needed\n",
    "            # and update self.best_mean_reward\n",
    "\n",
    "            \n",
    "\n",
    "            # ====================== #\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Best mean reward: 328.00\n",
      "Best mean reward: 332.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 47       |\n",
      "|    ep_rew_mean        | 47       |\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.596   |\n",
      "|    explained_variance | -0.0575  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.09     |\n",
      "|    value_loss         | 9.74     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 54.3     |\n",
      "|    ep_rew_mean        | 54.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.587   |\n",
      "|    explained_variance | -0.0785  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 7.5      |\n",
      "------------------------------------\n",
      "Best mean reward: 334.00\n",
      "Best mean reward: 395.00\n",
      "Best mean reward: 577.00\n",
      "Best mean reward: 934.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 56       |\n",
      "|    ep_rew_mean        | 56       |\n",
      "| time/                 |          |\n",
      "|    fps                | 434      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.665   |\n",
      "|    explained_variance | -0.00688 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.11     |\n",
      "|    value_loss         | 6.53     |\n",
      "------------------------------------\n",
      "Best mean reward: 1133.00\n",
      "Best mean reward: 1757.00\n",
      "Best mean reward: 1803.00\n",
      "Best mean reward: 2055.00\n",
      "Best mean reward: 2324.00\n",
      "Best mean reward: 6737.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.3     |\n",
      "|    ep_rew_mean        | 69.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 216      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.59    |\n",
      "|    explained_variance | -0.00268 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.972    |\n",
      "|    value_loss         | 5.6      |\n",
      "------------------------------------\n",
      "Best mean reward: 13387.00\n",
      "Best mean reward: 27387.00\n",
      "Best mean reward: 1397751.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb Celda 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m A2C(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     env\u001b[39m=\u001b[39menv,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# ====================== #\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Train the RL model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39mint\u001b[39;49m(\u001b[39m100_000\u001b[39;49m), callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/a2c/a2c.py:194\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m: SelfA2C,\n\u001b[1;32m    187\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    195\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    196\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    197\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    198\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    199\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    200\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    183\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 184\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "\u001b[1;32m/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb Celda 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m total_ep_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done: \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     action, a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(obs,deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     obs,reward,done, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jozaita/Documents/Stable_baselines_tutorial/callbacks_hyperparameters_tuning.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     total_ep_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:555\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    542\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/policies.py:344\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39mGet the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39mIncludes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39m    (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_training_mode(\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    346\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    348\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/policies.py:209\u001b[0m, in \u001b[0;36mBaseModel.set_training_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_training_mode\u001b[39m(\u001b[39mself\u001b[39m, mode: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    Put the policy in either training or evaluation mode.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m    :param mode: if true, set to training mode, else set to evaluation mode\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(mode)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2398\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2397\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2398\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2399\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2398\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2397\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2398\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2399\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2398\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2397\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2398\u001b[0m     module\u001b[39m.\u001b[39mtrain(mode)\n\u001b[1;32m   2399\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Env used for training\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# Env for evaluating the agent\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# === YOUR CODE HERE ===#\n",
    "# Create the callback object\n",
    "callback = EvalCallback(eval_env=eval_env)\n",
    "\n",
    "# Create the RL model\n",
    "\n",
    "\n",
    "model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# ====================== #\n",
    "\n",
    "# Train the RL model\n",
    "model.learn(int(100_000), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
